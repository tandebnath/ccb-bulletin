{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk 6 - Without Staff (WOS) - Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers issues from the Center for Children's Books. The primary differentiator is that, although every issue in this section has a reviewing staff for each book review, none of these issues contain a list of reviewing staff for that issueâ€”thus, \"without staff\". Furthermore, in these issues, the coded symbol appears right between the metadata text and the review text for any given book review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first preprocessing step involves identifying the last instance of the line \"New Titles for Children and Young People.\" We specifically look for the last instance because an issue can sometimes have multiple occurrences of this line in its introduction. Regardless of the number of instances, the introduction always ends with this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_introduction(source_dir, target_dir, target_line):\n",
    "    \"\"\"\n",
    "    Processes text files by removing the introduction up to and including the last instance \n",
    "    of the target line, then saves the modified content to the target directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The directory containing the source text files.\n",
    "        target_dir (str): The directory to save the processed text files.\n",
    "        target_line (str): The line that marks the end of the introduction.\n",
    "    \"\"\"\n",
    "    # Create target directory if it doesn't exist\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize the counter for the target line occurrences and a list for files without the line\n",
    "    line_count = 0\n",
    "    files_without_line = []\n",
    "    \n",
    "    # Process each file in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct full paths for source and target files\n",
    "            source_path = os.path.join(source_dir, filename)\n",
    "            target_path = os.path.join(target_dir, filename)\n",
    "            \n",
    "            # Read the content of the source file\n",
    "            with open(source_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.readlines()\n",
    "            \n",
    "            # Find the last occurrence of the target line\n",
    "            last_index = -1\n",
    "            for i, line in enumerate(content):\n",
    "                if target_line in line:\n",
    "                    last_index = i\n",
    "                    line_count += 1\n",
    "            \n",
    "            # Process the file based on the presence of the target line\n",
    "            if last_index != -1:\n",
    "                # Write content after the last occurrence of the target line to the new file\n",
    "                with open(target_path, 'w', encoding='utf-8') as file:\n",
    "                    file.writelines(content[last_index + 1:])\n",
    "            else:\n",
    "                # Add the filename to the list if the target line was not found\n",
    "                files_without_line.append(filename)\n",
    "            \n",
    "            # Print the processing message\n",
    "            print(f\"Processed {filename}\")\n",
    "    \n",
    "    # Print the total number of occurrences of the target line\n",
    "    print(f\"Total number of '{target_line}' occurrences found: {line_count}\")\n",
    "    # Print the names of files that did not contain the target line\n",
    "    if files_without_line:\n",
    "        print(\"Files without the target line:\")\n",
    "        for file_name in files_without_line:\n",
    "            print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory and line to process\n",
    "source_directory = 'TXT'\n",
    "target_directory = 'TXT_first_format'\n",
    "search_line = \"New Titles for Children and Young People\"\n",
    "\n",
    "# Call the function\n",
    "remove_introduction(source_directory, target_directory, search_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The second preprocessing step involves manually removing the extraneous content that appears after the completion of book reviews in every issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The third preprocessing step is to remove any instances of lines that contain the page number. In almost every case, the page number is the only content present in the given line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_page_number_lines(source_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Removes lines containing page numbers from text files in the source directory \n",
    "    and saves the processed content to the target directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The directory containing the source text files.\n",
    "        target_dir (str): The directory to save the processed text files.\n",
    "    \"\"\"\n",
    "    # Create target directory if it doesn't exist\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Define the regex pattern to match lines with page numbers\n",
    "    page_number_regex = re.compile(r\"^\\s*(\\[\\s*\\d{1,3}\\s*\\]|\\[\\s*\\d{1,3})\")\n",
    "\n",
    "    # Iterate through each file in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct full paths for source and target files\n",
    "            source_path = os.path.join(source_dir, filename)\n",
    "            target_path = os.path.join(target_dir, filename)\n",
    "            \n",
    "            # Read the content of the source file\n",
    "            with open(source_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.readlines()\n",
    "            \n",
    "            # Create a new list to store content without lines containing page numbers\n",
    "            new_content = []\n",
    "            for line in content:\n",
    "                stripped_line = line.strip()\n",
    "                if not page_number_regex.match(stripped_line):\n",
    "                    new_content.append(line)\n",
    "            \n",
    "            # Write the processed content to the new file\n",
    "            with open(target_path, 'w', encoding='utf-8') as file:\n",
    "                file.writelines(new_content)\n",
    "            \n",
    "            # Print the processing message\n",
    "            print(f\"Processed {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output folder paths\n",
    "input_folder = 'TXT_second_format'\n",
    "output_folder = 'TXT_third_format'\n",
    "\n",
    "# Run the processing\n",
    "remove_page_number_lines(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The fourth preprocessing step is to remove any instances of lines that contain \"C.U.\" and \"D.V\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cu_dv_lines(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Processes a text file by removing lines that start with \"C.U.\" or \"D.V.\" and,\n",
    "    in specific cases, the lines that follow them.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): The path to the input text file.\n",
    "        output_file_path (str): The path to save the processed text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    output_lines = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if line.startswith(\"C.U.\"):\n",
    "            # Check if the line 2 lines after this starts with \"D.V.\"\n",
    "            if i + 2 < len(lines) and lines[i + 2].strip().startswith(\"D.V.\"):\n",
    "                # Skip the \"C.U.\" line, the line after it, and the \"D.V.\" line\n",
    "                i += 3\n",
    "            else:\n",
    "                # Skip the \"C.U.\" line\n",
    "                i += 1\n",
    "        elif line.startswith(\"D.V.\"):\n",
    "            # Skip the \"D.V.\" line\n",
    "            i += 1\n",
    "        else:\n",
    "            output_lines.append(lines[i])\n",
    "            i += 1\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output folder paths\n",
    "input_folder_path = 'TXT_third_format'\n",
    "output_folder_path = 'TXT_fourth_format'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# Process each txt file in the input folder\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "        output_file_path = os.path.join(output_folder_path, filename)\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "        \n",
    "        if os.path.exists(input_file_path):\n",
    "            try:\n",
    "                remove_cu_dv_lines(input_file_path, output_file_path)\n",
    "                print(f'Processed and saved: {output_file_path}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {input_file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Text file not found: {input_file_path}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The fifth preprocessing step involves moving any line containing a coded symbol to the end of its respective metadata content. By \"the end,\" we mean making it the very last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coded symbols and grade year pattern\n",
    "coded_symbols = [\"R*\", \"R\", \"Ad\", \"M\", \"NR\", \"SpC\", \"SpR\"]\n",
    "coded_symbols_pattern = re.compile(r'^\\s*(' + '|'.join(re.escape(symbol) for symbol in coded_symbols) + r')\\s*$')\n",
    "grade_year_pattern = re.compile(r'^\\s*(?:[Ka-zA-Z]-\\d+|\\d+-[a-zA-Z]*|\\d+-\\d*|\\d+-|.*(?:Gr\\.|yrs\\.).*)\\s*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines) - 1:\n",
    "        line = lines[i].strip()\n",
    "        next_line = lines[i + 1].strip()\n",
    "        \n",
    "        if coded_symbols_pattern.match(line):\n",
    "            if grade_year_pattern.match(next_line) or '$' in next_line:\n",
    "                # Interchange the lines\n",
    "                lines[i], lines[i + 1] = lines[i + 1], lines[i]\n",
    "                i += 2  # Move past the next line\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output folder paths\n",
    "input_folder_path = 'TXT_fourth_format'\n",
    "output_folder_path = 'TXT_fifth_format'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# Process each txt file in the input folder\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "        output_file_path = os.path.join(output_folder_path, filename)\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "        \n",
    "        if os.path.exists(input_file_path):\n",
    "            try:\n",
    "                process_file(input_file_path, output_file_path)\n",
    "                print(f'Processed and saved: {output_file_path}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {input_file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Text file not found: {input_file_path}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Our final preprocessing step involves formatting the file names, to only include volumeNumber_issueNumber_year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_file_names(source_dir, target_dir):\n",
    "    # Create target directory if it doesn't exist\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate through each file in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Split the filename and extract the relevant parts\n",
    "            parts = filename.split('_')\n",
    "            new_filename = '_'.join(parts[7:10]) + '.txt'  # Construct the new filename from parts\n",
    "\n",
    "            # Construct full paths to source and target files\n",
    "            source_path = os.path.join(source_dir, filename)\n",
    "            target_path = os.path.join(target_dir, new_filename)\n",
    "            \n",
    "            # Copy the file with the new name to the target directory\n",
    "            shutil.copy(source_path, target_path)\n",
    "            print(f\"Processed and renamed {filename} to {new_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the source and target directories\n",
    "source_directory = 'TXT_fifth_format'\n",
    "target_directory = 'TXT_preprocessed'\n",
    "\n",
    "# Call the function\n",
    "format_file_names(source_directory, target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the actual number of reviews per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_actual_number_of_reviews(source_dir, output_file, coded_symbols):\n",
    "    # Initialize a list to hold the results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate through each file in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_id = filename.replace('.txt', '')  # Remove the .txt part for the ID\n",
    "            symbol_count = 0\n",
    "            \n",
    "            # Construct full path to the file\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            \n",
    "            # Open and read the file to count the coded symbols\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    if line.strip() in coded_symbols:\n",
    "                        symbol_count += 1\n",
    "            \n",
    "            # Append the result for this file to the results list\n",
    "            results.append([file_id, symbol_count])\n",
    "    \n",
    "    # Write results to the CSV file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['id', 'count'])  # Write column names\n",
    "        writer.writerows(results)\n",
    "    print(\"CSV file has been created with the count of coded symbols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the source directory and output file path\n",
    "source_directory = 'TXT_preprocessed'\n",
    "output_csv_file = 'actual_number_of_reviews.csv'\n",
    "\n",
    "# Call the function\n",
    "count_actual_number_of_reviews(source_directory, output_csv_file, coded_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be performing 2 steps here: \n",
    "1. Divide each issue into (0, n) number of book reviews\n",
    "2. Subsequently, bifurcate each book review into:\n",
    "   1. metadata.txt: containing the metadata of the review\n",
    "   2. review.txt: containing the review portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First, let's find the books in every issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex patterns\n",
    "author_regex = re.compile(r\"^\\s*([A-Za-z,-]+(?:\\s[A-Za-z,-]+)*)(?:\\s*\\([^)]*\\))?\\.\")\n",
    "reviewer_regex = re.compile(r\".*\\b([A-Z]{2,3})\\.?$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_books(input_file_path, output_folder_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    book_counter = 0\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        # Find the start of the book review\n",
    "        if author_regex.match(lines[i]):\n",
    "            print(f\"Book start found at line {i}: {lines[i]}\")\n",
    "            book_counter += 1\n",
    "            book_content = []\n",
    "            # Collect the book content until the reviewer line\n",
    "            while i < len(lines) and not reviewer_regex.match(lines[i]):\n",
    "                book_content.append(lines[i].strip())\n",
    "                i += 1\n",
    "            # Include the reviewer line in the book content\n",
    "            if i < len(lines) and reviewer_regex.match(lines[i]):\n",
    "                book_content.append(lines[i].strip())\n",
    "                print(f\"Reviewer found at line {i}: {lines[i]}\")\n",
    "                i += 1\n",
    "\n",
    "            # Create folder for the current book\n",
    "            book_folder = os.path.join(output_folder_path, f'Book_{book_counter}')\n",
    "            os.makedirs(book_folder, exist_ok=True)\n",
    "            \n",
    "            # Write book content to book.txt\n",
    "            with open(os.path.join(book_folder, 'book.txt'), 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n'.join(book_content))\n",
    "            print(f\"Book {book_counter} saved with {len(book_content)} lines.\")\n",
    "        else:\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output folder paths\n",
    "input_folder_path = 'TXT_preprocessed'\n",
    "output_parent_folder = 'Books_identified'\n",
    "\n",
    "# Create the output parent folder if it doesn't exist\n",
    "if not os.path.exists(output_parent_folder):\n",
    "    os.makedirs(output_parent_folder)\n",
    "\n",
    "# Process each txt file in the input folder\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "        output_folder_path = os.path.join(output_parent_folder, filename.replace('.txt', ''))\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "        \n",
    "        if os.path.exists(input_file_path):\n",
    "            try:\n",
    "                identify_books(input_file_path, output_folder_path)\n",
    "                print(f'Processed and saved: {output_folder_path}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {input_file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Text file not found: {input_file_path}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Next, lets divide each book into metadata and review text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_symbols = [\"R*\", \"R\", \"Ad\", \"M\", \"NR\", \"SpC\", \"SpR\"]\n",
    "# Define regex patterns\n",
    "coded_symbols_pattern = re.compile(r'^\\s*(' + '|'.join(re.escape(symbol) for symbol in coded_symbols) + r')\\s*$')\n",
    "reviewer_regex = re.compile(r\"\\b([A-Z]{2,3})\\.?$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_move_reviewer(book_folder):\n",
    "    book_file_path = os.path.join(book_folder, 'book.txt')\n",
    "    with open(book_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    metadata = []\n",
    "    review = []\n",
    "    found_coded_symbol = False\n",
    "    reviewer_initials = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if not found_coded_symbol:\n",
    "            metadata.append(line.strip())\n",
    "            if coded_symbols_pattern.match(line):\n",
    "                found_coded_symbol = True\n",
    "        else:\n",
    "            review.append(line.strip())\n",
    "\n",
    "    # Move reviewer initials to metadata\n",
    "    if review:\n",
    "        last_line = review[-1]\n",
    "        match = reviewer_regex.search(last_line)\n",
    "        if match:\n",
    "            reviewer_initials = match.group(1)\n",
    "            review[-1] = reviewer_regex.sub(\"\", last_line).strip()\n",
    "\n",
    "    # Write metadata to metadata.txt\n",
    "    if reviewer_initials:\n",
    "        metadata.append(reviewer_initials)\n",
    "    \n",
    "    with open(os.path.join(book_folder, 'metadata.txt'), 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(metadata))\n",
    "    \n",
    "    # Write review to review.txt\n",
    "    with open(os.path.join(book_folder, 'review.txt'), 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(review))\n",
    "    \n",
    "    # Remove the original book.txt file\n",
    "    os.remove(book_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output folder paths\n",
    "input_folder_path = 'Books_identified'\n",
    "output_parent_folder = 'Books_bifurcated'\n",
    "\n",
    "# Create the output parent folder if it doesn't exist\n",
    "if not os.path.exists(output_parent_folder):\n",
    "    os.makedirs(output_parent_folder)\n",
    "\n",
    "# Process each book folder in the input folder\n",
    "for folder_name in os.listdir(input_folder_path):\n",
    "    book_folder_path = os.path.join(input_folder_path, folder_name)\n",
    "    if os.path.isdir(book_folder_path):\n",
    "        output_folder_path = os.path.join(output_parent_folder, folder_name)\n",
    "        if not os.path.exists(output_folder_path):\n",
    "            os.makedirs(output_folder_path)\n",
    "        \n",
    "        for book_name in os.listdir(book_folder_path):\n",
    "            book_path = os.path.join(book_folder_path, book_name)\n",
    "            if os.path.isdir(book_path):\n",
    "                output_book_folder = os.path.join(output_folder_path, book_name)\n",
    "                if not os.path.exists(output_book_folder):\n",
    "                    os.makedirs(output_book_folder)\n",
    "                \n",
    "                # Copy the book.txt file to the output folder\n",
    "                book_file_path = os.path.join(book_path, 'book.txt')\n",
    "                if os.path.exists(book_file_path):\n",
    "                    with open(book_file_path, 'r', encoding='utf-8') as file:\n",
    "                        book_content = file.readlines()\n",
    "                    \n",
    "                    output_book_file_path = os.path.join(output_book_folder, 'book.txt')\n",
    "                    with open(output_book_file_path, 'w', encoding='utf-8') as file:\n",
    "                        file.writelines(book_content)\n",
    "                    \n",
    "                    # Divide the book into metadata.txt and review.txt, then move reviewer initials\n",
    "                    divide_and_move_reviewer(output_book_folder)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ### Calculating the number of reviews which could not be captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets count the obtained number of book reviews per issue, and save it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_obtained_number_of_reviews(source_dir, output_file):\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each issue folder in the source directory\n",
    "    for issue_folder in os.listdir(source_dir):\n",
    "        issue_path = os.path.join(source_dir, issue_folder)\n",
    "        if os.path.isdir(issue_path):  # Ensure it's a directory\n",
    "            count = len([name for name in os.listdir(issue_path) if 'Book_' in name and os.path.isdir(os.path.join(issue_path, name))])\n",
    "            results.append([issue_folder, count])\n",
    "    \n",
    "    # Write the results to a CSV file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['id', 'count'])\n",
    "        writer.writerows(results)\n",
    "    print(\"CSV file has been created with the count of book folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created with the count of book folders.\n"
     ]
    }
   ],
   "source": [
    "# Specify the source directory and output file path\n",
    "source_directory = 'Books_identified'\n",
    "output_csv_file = 'obtained_number_of_reviews.csv'\n",
    "\n",
    "# Call the function\n",
    "count_obtained_number_of_reviews(source_directory, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets calculate the difference, per file, between the actual number of reviews and the obtained number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(actual_file, obtained_file, output_file):\n",
    "    # Load data from CSV files\n",
    "    def load_data(filename):\n",
    "        with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)  # Skip the header\n",
    "            return {rows[0]: int(rows[1]) for rows in reader}\n",
    "\n",
    "    actual_counts = load_data(actual_file)\n",
    "    obtained_counts = load_data(obtained_file)\n",
    "    \n",
    "    # Compare counts and calculate differences\n",
    "    results = []\n",
    "    for id in actual_counts:\n",
    "        actual_count = actual_counts.get(id, 0)\n",
    "        obtained_count = obtained_counts.get(id, 0)\n",
    "        difference = actual_count - obtained_count\n",
    "        results.append([id, difference])\n",
    "    \n",
    "    # Write the results to a CSV file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['id', 'difference'])\n",
    "        writer.writerows(results)\n",
    "    print(\"Comparison CSV file has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison CSV file has been created.\n"
     ]
    }
   ],
   "source": [
    "# Specify the file paths\n",
    "actual_csv_file = 'actual_number_of_reviews.csv'\n",
    "obtained_csv_file = 'obtained_number_of_reviews.csv'\n",
    "output_comparison_csv = 'review_count_comparison.csv'\n",
    "\n",
    "# Call the function\n",
    "calculate_difference(actual_csv_file, obtained_csv_file, output_comparison_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Next, let's flatten the bifurcated books folder to aid in further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_folders(source_dir, target_dir):\n",
    "    # Ensure the target directory exists\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each issue folder\n",
    "    for issue_folder in os.listdir(source_dir):\n",
    "        issue_path = os.path.join(source_dir, issue_folder)\n",
    "        \n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(issue_path):\n",
    "            # Iterate over each book folder inside the issue folder\n",
    "            for book_folder in os.listdir(issue_path):\n",
    "                book_path = os.path.join(issue_path, book_folder)\n",
    "                \n",
    "                if os.path.isdir(book_path) and book_folder.startswith('Book_'):\n",
    "                    # Construct the new folder name based on the issue and book number\n",
    "                    book_number = book_folder.split('_')[1]\n",
    "                    new_folder_name = f\"{issue_folder}_{book_number.zfill(3)}\"\n",
    "                    new_folder_path = os.path.join(target_dir, new_folder_name)\n",
    "                    \n",
    "                    # Create the new folder\n",
    "                    os.makedirs(new_folder_path, exist_ok=True)\n",
    "                    \n",
    "                    # Copy the files (review.txt and metadata.txt) to the new folder\n",
    "                    for file in os.listdir(book_path):\n",
    "                        src_file_path = os.path.join(book_path, file)\n",
    "                        dst_file_path = os.path.join(new_folder_path, file)\n",
    "                        shutil.copy(src_file_path, dst_file_path)\n",
    "    \n",
    "    print(\"Folders have been flattened and files have been copied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders have been flattened and files have been copied.\n"
     ]
    }
   ],
   "source": [
    "# Specify the source directory and target directory\n",
    "source_directory = 'Books_bifurcated'\n",
    "target_directory = 'Books_bifurcated_flattened'\n",
    "\n",
    "# Call the function to flatten the folders\n",
    "flatten_folders(source_directory, target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the parent folders\n",
    "parent_dir = \"Books_bifurcated_flattened\"\n",
    "\n",
    "# Base Excel file path\n",
    "base_excel_file_path = \"consolidated_metadata.xlsx\"\n",
    "\n",
    "# Define regex patterns for extracting data\n",
    "author_regex = re.compile(r\"^(.*?), (.*?)(?=\\.)\")\n",
    "book_name_regex = re.compile(r\"\\. (.*?)[.;]\")\n",
    "publisher_year_regex = re.compile(r\"\\.\\s+([^.,]+), (\\d{4})\")\n",
    "price_regex = re.compile(r\"(\\$\\d+\\.?\\d*)\")\n",
    "isbn_regex = re.compile(r\"(ISBN [\\d-]+(?:,? ?[\\d-]+)*).*?[.;]\", re.DOTALL)\n",
    "code_regex = re.compile(r\"^(?:[*]|R|Ad|M|NR|SpC|SpR)$\", re.MULTILINE)\n",
    "reviewer_regex = re.compile(r\"^([A-Z]{2,3})\\.*$\", re.MULTILINE)\n",
    "grade_years_regex = re.compile(r\"^(.*(?:Gr\\.|yrs\\.).*)$\", re.MULTILINE)\n",
    "illustrator_regex = re.compile(r\"(illus\\..*?)\\.\")\n",
    "pages_regex = re.compile(r\"\\b(\\d{1,3}p)\\b\")\n",
    "square_bracket_regex = re.compile(\n",
    "    r\"\\[\\d{1,2}\\]\"\n",
    ")  # Regex to find numbers within square brackets\n",
    "\n",
    "# Initialize lists for storing data rows and various conditions\n",
    "rows = []\n",
    "empty_book_name = []\n",
    "empty_author_name = []\n",
    "empty_publisher = []\n",
    "empty_year = []\n",
    "erroneous_price = []\n",
    "multiple_initials = []\n",
    "\n",
    "# Iterate over each folder in the parent directory\n",
    "for folder in os.listdir(parent_dir):\n",
    "    folder_path = os.path.join(parent_dir, folder)\n",
    "    metadata_path = os.path.join(folder_path, \"metadata.txt\")\n",
    "\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "            metadata = meta_file.read()\n",
    "            temp_metadata = \" \".join(\n",
    "                metadata.splitlines()\n",
    "            )  # Create flattened metadata for publisher and year\n",
    "\n",
    "            # Remove lines containing numbers within square brackets from both metadata and temp_metadata\n",
    "            metadata = re.sub(square_bracket_regex, \"\", metadata)\n",
    "            temp_metadata = re.sub(square_bracket_regex, \"\", temp_metadata)\n",
    "\n",
    "            # Extract information using regex\n",
    "            author_match = author_regex.search(metadata)\n",
    "            book_name_match = book_name_regex.search(metadata)\n",
    "            publisher_year_match = publisher_year_regex.search(temp_metadata)\n",
    "            price_matches = price_regex.findall(metadata)\n",
    "            isbn_matches = isbn_regex.findall(metadata)\n",
    "            code_match = code_regex.search(metadata)\n",
    "            reviewer_match = reviewer_regex.search(metadata)\n",
    "            grade_years_matches = grade_years_regex.findall(metadata)\n",
    "            illustrator_match = illustrator_regex.search(metadata)\n",
    "            pages_match = pages_regex.search(metadata)\n",
    "\n",
    "            # Prepare data row\n",
    "            row = [\n",
    "                folder,\n",
    "                publisher_year_match.group(2) if publisher_year_match else \"\",\n",
    "                book_name_match.group(1) if book_name_match else \"\",\n",
    "                (\n",
    "                    author_match.group(1) + \", \" + author_match.group(2)\n",
    "                    if author_match and author_match.groups()\n",
    "                    else \"\"\n",
    "                ),\n",
    "                publisher_year_match.group(1) if publisher_year_match else \"\",\n",
    "                \", \".join(price_matches),\n",
    "                \", \".join(isbn_matches).replace(\"\\n\", \" \"),\n",
    "                code_match.group(0) if code_match else \"\",\n",
    "                reviewer_match.group(1) if reviewer_match else \"\",\n",
    "                \", \".join(set(grade_years_matches)),\n",
    "                illustrator_match.group(0) if illustrator_match else \"\",\n",
    "                pages_match.group(0) if pages_match else \"\",\n",
    "            ]\n",
    "\n",
    "            # Add row to main list and check conditions for filtering\n",
    "            rows.append(row)\n",
    "            if not row[2]:  # Empty Book Name\n",
    "                empty_book_name.append(row)\n",
    "            if not row[3]:  # Empty Author Name\n",
    "                empty_author_name.append(row)\n",
    "            if not row[4]:  # Empty Publisher\n",
    "                empty_publisher.append(row)\n",
    "            if not row[1]:  # Empty Year\n",
    "                empty_year.append(row)\n",
    "            if row[5].count(\"$\") > 2:  # Erroneous Price\n",
    "                erroneous_price.append(row)\n",
    "            elif len(row[2]) == 1:  # Book Names only 1 letter long\n",
    "                multiple_initials.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before filtering: 3784\n",
      "Rows with empty book name: 20\n",
      "Rows with empty author name: 85\n",
      "Rows with empty publisher: 55\n",
      "Rows with empty year: 55\n",
      "Rows with erroneous price: 6\n",
      "Rows with book names only 1 letter long: 58\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert lists to pandas DataFrames\n",
    "df_all_rows = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"ID\",\n",
    "        \"Year\",\n",
    "        \"Book Name\",\n",
    "        \"Author Name\",\n",
    "        \"Publisher\",\n",
    "        \"Price\",\n",
    "        \"ISBN\",\n",
    "        \"Code\",\n",
    "        \"Reviewer\",\n",
    "        \"Grade Years\",\n",
    "        \"Illustrator\",\n",
    "        \"Pages\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_empty_book_name = pd.DataFrame(empty_book_name, columns=df_all_rows.columns)\n",
    "df_empty_author_name = pd.DataFrame(empty_author_name, columns=df_all_rows.columns)\n",
    "df_empty_publisher = pd.DataFrame(empty_publisher, columns=df_all_rows.columns)\n",
    "df_empty_year = pd.DataFrame(empty_year, columns=df_all_rows.columns)\n",
    "df_erroneous_price = pd.DataFrame(erroneous_price, columns=df_all_rows.columns)\n",
    "df_multiple_initials = pd.DataFrame(multiple_initials, columns=df_all_rows.columns)\n",
    "\n",
    "# Filter the main dataframe to exclude rows present in other dataframes\n",
    "filtered_rows_df = df_all_rows[\n",
    "    ~df_all_rows[\"ID\"].isin(\n",
    "        pd.concat(\n",
    "            [\n",
    "                df_empty_author_name[\"ID\"],\n",
    "                df_erroneous_price[\"ID\"],\n",
    "                df_empty_publisher[\"ID\"],\n",
    "                df_empty_year[\"ID\"],\n",
    "                df_erroneous_price[\"ID\"],\n",
    "                df_multiple_initials[\"ID\"],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# Write to Excel file with multiple sheets using pandas\n",
    "with pd.ExcelWriter(base_excel_file_path, engine=\"openpyxl\") as writer:\n",
    "    df_all_rows.to_excel(writer, sheet_name=\"all_rows\", index=False)\n",
    "    filtered_rows_df.to_excel(writer, sheet_name=\"complete\", index=False)\n",
    "    df_empty_book_name.to_excel(writer, sheet_name=\"book_missing\", index=False)\n",
    "    df_empty_author_name.to_excel(writer, sheet_name=\"author_missing\", index=False)\n",
    "    df_empty_publisher.to_excel(writer, sheet_name=\"publisher_missing\", index=False)\n",
    "    df_empty_year.to_excel(writer, sheet_name=\"year_missing\", index=False)\n",
    "    df_erroneous_price.to_excel(writer, sheet_name=\"price_multiple\", index=False)\n",
    "    df_multiple_initials.to_excel(writer, sheet_name=\"initials_multiple\", index=False)\n",
    "\n",
    "# Output the counts\n",
    "print(f\"Total rows before filtering: {len(rows)}\")\n",
    "print(f\"Rows with empty book name: {len(empty_book_name)}\")\n",
    "print(f\"Rows with empty author name: {len(empty_author_name)}\")\n",
    "print(f\"Rows with empty publisher: {len(empty_publisher)}\")\n",
    "print(f\"Rows with empty year: {len(empty_year)}\")\n",
    "print(f\"Rows with erroneous price: {len(erroneous_price)}\")\n",
    "print(f\"Rows with book names only 1 letter long: {len(multiple_initials)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Text Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "excel_file_path = 'complete.xlsx'\n",
    "df = pd.read_excel(excel_file_path, sheet_name='complete')\n",
    "\n",
    "# Path to the folder containing bifurcated folders\n",
    "bifurcated_folder_path = 'Bifurcated_flattened'\n",
    "\n",
    "# Create a folder to store reviews if it doesn't exist\n",
    "reviews_folder_path = 'Reviews'\n",
    "if not os.path.exists(reviews_folder_path):\n",
    "    os.makedirs(reviews_folder_path)\n",
    "\n",
    "# Iterate through each ID in the Excel sheet\n",
    "for index, row in df.iterrows():\n",
    "    id_name = str(row['ID'])  # Assuming 'ID' is the column name\n",
    "    \n",
    "    # Check if the folder exists for the current ID\n",
    "    folder_path = os.path.join(bifurcated_folder_path, id_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        # Check if review.txt exists in the folder\n",
    "        review_file_path = os.path.join(folder_path, 'review.txt')\n",
    "        if os.path.exists(review_file_path):\n",
    "            # Rename and move the review.txt file to the Reviews folder\n",
    "            new_file_name = id_name + '.txt'\n",
    "            new_file_path = os.path.join(reviews_folder_path, new_file_name)\n",
    "            shutil.copy(review_file_path, new_file_path)\n",
    "            print(f\"Review file for ID {id_name} copied and renamed.\")\n",
    "        else:\n",
    "            print(f\"Review file not found for ID {id_name}.\")\n",
    "    else:\n",
    "        print(f\"Folder not found for ID {id_name}.\")\n",
    "\n",
    "print(\"Process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
